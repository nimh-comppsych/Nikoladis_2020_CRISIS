{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import spearmanr, pearsonr\n",
    "from sklearn.model_selection import train_test_split\n",
    "from time import time\n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "import nilearn\n",
    "import nibabel as nib\n",
    "from nilearn.input_data import NiftiLabelsMasker\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "from igraph import *\n",
    "\n",
    "\n",
    "def match(C):\n",
    "    \"\"\"\n",
    "    Replication of R's match function\n",
    "    \"\"\"\n",
    "    \n",
    "    u, ui = np.unique(C, return_index=True)\n",
    "    ui = sorted(ui)\n",
    "    newC = np.zeros(C.shape)\n",
    "    for i in range(len(ui)):\n",
    "        newC[np.where(C == C[ui[i]])[0]] = i + 1\n",
    "        \n",
    "    return newC\n",
    "\n",
    "\n",
    "def outer_equal(x):\n",
    "    \"\"\"\n",
    "    Replication of R's outer function with FUN param '=='\n",
    "    \"\"\"\n",
    "    \n",
    "    res = np.zeros((x.shape[0], x.shape[0]))\n",
    "    for i in range(x.shape[0]):\n",
    "        res[i,:] = x == x[i]\n",
    "    \n",
    "    return res\n",
    "\n",
    "\n",
    "def hadamard(nearest):\n",
    "    \"\"\"\n",
    "    Transform data into Hadamard distance matrix\n",
    "    \"\"\"\n",
    "    \n",
    "    common = nearest.T @ nearest\n",
    "    ranks = np.outer(np.diag(common), np.ones(nearest.shape[0]))\n",
    "    neighborUnion = ranks + ranks.T - common\n",
    "    G = common / neighborUnion\n",
    "    np.fill_diagonal(G, 0)\n",
    "    \n",
    "    return G\n",
    "\n",
    "\n",
    "def topMax(x, N):\n",
    "    \"\"\"\n",
    "    find Nth largest number in an array\n",
    "    \"\"\"\n",
    "    \n",
    "    L = len(x)\n",
    "    assert N < L, 'Number of neighbors cannot be larger than length of data'\n",
    "    \n",
    "    while L != 1:\n",
    "        initial_guess = x[0]\n",
    "        top_list = x[x > initial_guess]\n",
    "        bottom_list = x[x < initial_guess]\n",
    "        \n",
    "        topL = len(top_list)\n",
    "        bottomL = len(bottom_list)\n",
    "        \n",
    "        if (topL < N) and (L - bottomL >= N):\n",
    "            x = initial_guess\n",
    "            break\n",
    "        \n",
    "        if topL >= N:\n",
    "            x = top_list\n",
    "        else:\n",
    "            x = bottom_list\n",
    "            N = N - L + bottomL\n",
    "        L = len(x)\n",
    "    \n",
    "    return x\n",
    "\n",
    "\n",
    "def bottomMin(x, N):\n",
    "    \"\"\"\n",
    "    find Nth smallest number in an array\n",
    "    \"\"\"\n",
    "    \n",
    "    return np.round(topMax(x, len(x) - N + 1))\n",
    "\n",
    "\n",
    "def find_neighbors(D, k):\n",
    "    \"\"\"\n",
    "    Tranform distance matrix to binary k-nearest neighbors graph\n",
    "    \"\"\"\n",
    "    \n",
    "    nearest = np.zeros(D.shape)\n",
    "    for i in range(nearest.shape[1]):\n",
    "        nearest[:,i] = np.round(D[:,i]) <= bottomMin(np.round(D[:,i]), k)\n",
    "    \n",
    "    return nearest\n",
    "\n",
    "\n",
    "def modularity(G, C):\n",
    "    \"\"\"\n",
    "    Calculate graph's modularity\n",
    "    \"\"\"\n",
    "    \n",
    "    m = np.sum(G)\n",
    "    Ki = np.repeat(np.sum(G, axis=1), G.shape[1]).reshape(G.shape)\n",
    "    Kj = Ki.T\n",
    "    delta_function = outer_equal(C)\n",
    "    Q = (G - Ki * Kj / m) * delta_function / m\n",
    "    return np.sum(Q)\n",
    "\n",
    "\n",
    "def delta_modularity(G, C, i, m, Kj, newC=None):\n",
    "    \"\"\"\n",
    "    Calculate change in modularity by adding a node to a cluster or by removing it\n",
    "    \"\"\"\n",
    "    \n",
    "    c = np.copy(C)\n",
    "    \n",
    "    if newC is None: # removing a node from C\n",
    "        # removing a solitary node from a cluster does not change the modularity\n",
    "        if np.sum(c == c[i]) == 1:\n",
    "            return 0\n",
    "        newC = c[i]\n",
    "        c[i] = np.max(c) + 1\n",
    "        \n",
    "    I = c == newC\n",
    "    Ki = np.sum(G[i,:]) # the sum of all the edges connected to node i\n",
    "    Ki_in = np.sum(G[i,I]) # the sum of all the edges conneting node i to C\n",
    "    Kjs = np.sum(Kj[I])\n",
    "    deltaQ = Ki_in / m - Ki * Kjs / m**2\n",
    "    \n",
    "    return deltaQ * 2\n",
    "\n",
    "\n",
    "def louvain_step(G, C, O, Q=None):\n",
    "    \"\"\"\n",
    "    Run a single step of the Louvain algorithm\n",
    "    \"\"\"\n",
    "    \n",
    "    if not Q:\n",
    "        Q = modularity(G, C)\n",
    "        \n",
    "    m = np.sum(G)\n",
    "    Kj = np.sum(G, axis=1)\n",
    "    \n",
    "    for i in O:\n",
    "        reassign = np.array(list(set(C[G[i,:] > 0]).difference(set([C[i]]))))\n",
    "        \n",
    "        if len(reassign) != 0:\n",
    "            delta_remove = delta_modularity(G, C, i, m, Kj)\n",
    "\n",
    "            deltaQ = np.array([delta_modularity(G, C, i, m, Kj, newC=x) for x in reassign])\n",
    "\n",
    "            if np.max(deltaQ) - delta_remove > 0:\n",
    "                idx = np.argmax(deltaQ)\n",
    "                Q += np.max(deltaQ) - delta_remove\n",
    "                if reassign[idx] > 2389:\n",
    "                    print(i)\n",
    "                C[i] = reassign[idx]\n",
    "            if delta_remove < 0:\n",
    "                C[i] = np.max(C) + 1\n",
    "                Q -= delta_remove\n",
    "    \n",
    "    return C, Q\n",
    "\n",
    "\n",
    "def louvain(G, C, maxreps=100):\n",
    "    \"\"\"\n",
    "    Run Louvain algorithm\n",
    "    \"\"\"\n",
    "    \n",
    "    assert np.allclose(G, G.T), 'Input graph must be symmetric'\n",
    "    \n",
    "    order = np.random.permutation(G.shape[1])\n",
    "    Q = modularity(G, C)\n",
    "    \n",
    "    for i in range(maxreps):\n",
    "        C, Q = louvain_step(G, C, order, Q)\n",
    "        C = match(C)\n",
    "        \n",
    "        # run the second phase, trying to combine each cluster\n",
    "        n_clust = len(np.unique(C))\n",
    "        metaG = np.zeros((n_clust, n_clust))\n",
    "        for ci in range(1, n_clust + 1):\n",
    "            G_tmp = G[C == ci]\n",
    "            for cj in range(ci, n_clust + 1):\n",
    "                G_tmp2 = G_tmp[:,C == cj]\n",
    "                metaG[ci-1,cj-1] = np.sum(G_tmp2)\n",
    "                metaG[cj-1,ci-1] = metaG[ci-1,cj-1]\n",
    "                \n",
    "        metaC, metaQ = louvain_step(metaG, np.arange(n_clust) + 1, np.random.permutation(n_clust))\n",
    "        if metaQ - Q > 1e-15:\n",
    "            tempC = np.copy(C)\n",
    "            for ci in range(1, n_clust + 1):\n",
    "                tempC[C == ci] = metaC[ci-1]\n",
    "            C = match(tempC)\n",
    "            Q = metaQ\n",
    "        else:\n",
    "            break\n",
    "            \n",
    "    return C, Q\n",
    "\n",
    "\n",
    "def plot_community_profiles(X, communities, outfile=None):\n",
    "    \"\"\"\n",
    "    Create radar plot of average profiles of each detected community\n",
    "    \"\"\"\n",
    "    \n",
    "    # number of variables\n",
    "    categories = subset\n",
    "    N = len(categories)\n",
    "\n",
    "    # What will be the angle of each axis in the plot? (we divide the plot / number of variable)\n",
    "    angles = [n / float(N) * 2 * np.pi for n in range(N)]\n",
    "    angles += angles[:1]\n",
    "\n",
    "    # Initialise the spider plot\n",
    "    fig=plt.figure(figsize=(10,10))\n",
    "    ax = fig.add_subplot(111, polar=True)\n",
    "\n",
    "    # If you want the first axis to be on top:\n",
    "    ax.set_theta_offset(np.pi / 2)\n",
    "    ax.set_theta_direction(-1)\n",
    "\n",
    "    # Draw one axe per variable + add labels labels yet\n",
    "    plt.xticks(angles[:-1], categories)\n",
    "    pi = np.pi\n",
    "    for label,i in zip(ax.get_xticklabels(), range(0,len(angles))):\n",
    "        angle_rad = angles[i]\n",
    "        if angle_rad <= pi/2:\n",
    "            ha = 'left'\n",
    "            va = \"bottom\"\n",
    "            angle_text = angle_rad * (-180/pi) + 90\n",
    "        elif pi/2 < angle_rad <= pi:\n",
    "            ha = 'left'\n",
    "            va = \"top\"\n",
    "            angle_text = angle_rad * (-180/pi) + 90\n",
    "        elif pi < angle_rad <= (3*pi/2):\n",
    "            ha = 'right'\n",
    "            va = \"top\"  \n",
    "            angle_text = angle_rad * (-180/pi) - 90\n",
    "        else:\n",
    "            ha = 'right'\n",
    "            va = \"bottom\"\n",
    "            angle_text = angle_rad * (-180/pi) - 90\n",
    "        label.set_rotation(angle_text)\n",
    "        label.set_verticalalignment(va)\n",
    "        label.set_horizontalalignment(ha)\n",
    "\n",
    "    # Draw ylabels\n",
    "    ax.set_rlabel_position(0)\n",
    "    plt.yticks(np.linspace(50,70,5), color=\"k\", size=10)\n",
    "    plt.ylim(50,70)\n",
    "\n",
    "    # ------- PART 2: Add plots\n",
    "    comms, counts = np.unique(communities, return_counts=True)\n",
    "    for c, n in zip(comms, counts):\n",
    "        comm_X = X[communities == c,:]\n",
    "        comm_avg = np.mean(comm_X, axis=0)\n",
    "        values = comm_avg.tolist()\n",
    "        values += values[:1]\n",
    "        ax.plot(angles, values, linewidth=1.5, linestyle='solid', label=\"Community %d (N = %d)\" % (c, n))\n",
    "#         ax.fill(angles, values, 'b', alpha=0.1)\n",
    "    plt.legend(bbox_to_anchor=(1.1, 1.1), fontsize=10)\n",
    "    if outfile:\n",
    "        plt.savefig(outfile, bbox_inches='tight', dpi=300)\n",
    "        \n",
    "        \n",
    "def pheno_clust(filepath=None, subset=None, X=None, plot=True, outfile=None, repeats=50, verbose=True):\n",
    "    \"\"\"\n",
    "    Run entire phenoClust pipeline\n",
    "    \"\"\"\n",
    "    \n",
    "    assert filepath or X is not None, 'Either a matrix or a filepath to the data must be passed in'\n",
    "    \n",
    "    if verbose:\n",
    "        tic = time()\n",
    "    \n",
    "    if filepath:\n",
    "        assert subset, 'If filepath to dataframe is passed, you must also include the subset of columns that you intend to pass into the algorithm'\n",
    "        \n",
    "        df = pd.read_csv(filepath)\n",
    "        df = df[subset]\n",
    "        df.dropna(inplace=True)\n",
    "\n",
    "        X = np.array(df)\n",
    "        \n",
    "    X_Z = (X - np.mean(X, axis=0, keepdims=True)) / np.std(X, axis=0, dtype=np.float64, ddof=1, keepdims=True)\n",
    "    D, rho = spearmanr(X_Z, axis=1)\n",
    "    D = np.round((X_Z.shape[1]**3 - X_Z.shape[1]) * (1 - D) / 6).astype(np.int)\n",
    "    \n",
    "    b = np.ceil(np.log2(X_Z.shape[0]) + 1)\n",
    "    k = np.ceil(X_Z.shape[0] / b).astype(np.int)\n",
    "    nearest = find_neighbors(D, k + 1)\n",
    "    G = hadamard(nearest)\n",
    "    C = np.arange(G.shape[1]) + 1\n",
    "    \n",
    "    Q = -np.inf\n",
    "    for t in range(repeats):\n",
    "        C, newQ = louvain(G, C)\n",
    "        if newQ > Q:\n",
    "            Q = newQ\n",
    "            if verbose:\n",
    "                print('iter: %d/%d; Q = %.5f; # of communities: %d' % (t+1, repeats, Q, len(np.unique(C))))\n",
    "        else:\n",
    "            break\n",
    "    \n",
    "    if verbose:\n",
    "        toc = time()\n",
    "        print('PyPhenoClust completed in: %.2f seconds' % (toc - tic))\n",
    "    \n",
    "    if plot:\n",
    "        plot_community_profiles(X, C, outfile)\n",
    "    \n",
    "    return C, Q\n",
    "\n",
    "def bagging_adjacency_matrixes(csv_folder, split):\n",
    "    '''csv_folder: folder that contains all the csv output from python louvain.\n",
    "       split: string in the csv files' names that specifies the split eg.Split_1 '''\n",
    "    \n",
    "    my_data = [pd.read_csv(csv_folder+file,index_col=0) for file in os.listdir(csv_folder) if split in file] # list contains all the csv in dataframe format\n",
    "    \n",
    "    for i in range(len(my_data)): # loop over my_data\n",
    "        boot=my_data[i].drop_duplicates() #remove duplicates\n",
    "        df_merge = boot.merge(boot, on='cluster') # create adjacency matrix step1 \n",
    "        this_adj =pd.crosstab(df_merge.studyid_x, df_merge.studyid_y) # create adjacency matrix step2\n",
    "        np.fill_diagonal(this_adj.values, 0) #set the diagonal of the adjacency matrix to 0\n",
    "        \n",
    "        if i ==0:\n",
    "            adj_full=this_adj #if this is the first adjacency matrix set it as the initial adj_full \n",
    "        else:\n",
    "            adj_full=adj_full.add(this_adj,fill_value=0) # add this adjacency matrix to the full adjacency matrix\n",
    "\n",
    "\n",
    "        boot_mask=my_data[i].drop_duplicates()\n",
    "        boot_mask.cluster=boot_mask.cluster.replace(boot_mask.cluster.unique(),np.ones(boot_mask.cluster.unique().shape))#replace all clusters with 1\n",
    "        df_merge2 = boot_mask.merge(boot_mask, on='cluster')\n",
    "        mask_adj =pd.crosstab(df_merge2.studyid_x, df_merge2.studyid_y)\n",
    "        np.fill_diagonal(mask_adj.values, 0) #set the diagonal of the adjacency matrix to 0\n",
    "        if i ==0:\n",
    "            mask_full= mask_adj  #if this is the first adjacency matrix set it as the initial mask_full \n",
    "        else:\n",
    "            mask_full=mask_full.add( mask_adj,fill_value=0) # add this mask adjacency matrix to the full mask adjacency matrix\n",
    "    \n",
    "    \n",
    "    stab_full = adj_full.div(mask_full)\n",
    "    np.fill_diagonal(stab_full.values, 0)\n",
    "        \n",
    "    return adj_full, mask_full, stab_full\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Subset Variables will depend on the dateset of interest.\n",
    "\n",
    "##Life Changes Adult Parent: \n",
    "#subset = [\"R_positivechange\",\"R_inpersonconvo_bin\",\"R_timeoutside\",\"R_restrictionsstress\",\"R_contactschanged\",\n",
    "#          \"R_difficultydistancing\",\"R_familychange\",\"R_familychangestress\", \"R_friendschange\",\"R_friendchangestress\",\n",
    "#          \"R_difficultycancellations\",\"R_financedifficulty\",\"R_livingdifficulty\",\"R_foodsecurity\"]  #Age and Sex Residuals\n",
    "\n",
    "#Prior Habits Adult: \n",
    "#subset = [\"R_bedtimeweekdays\", \"R_bedtimeweekends\",\"R_hoursofsleepweekdays\", \"R_hoursofsleepweekends\",\n",
    "#          \"R_exerciseprior\",\"R_outdoorsprior\",\"R_priortvmedia\",\"R_priorsocialmedia\",\"R_priorvideogames\",\n",
    "#          \"R_threemonthsalcohol\", \"R_threemonthsvaping\",\"R_threemonthstobacco\",\"R_threemonthsmarijuana\", \n",
    "#          \"R_threemonthsopiates\", \"R_threemonthsother\", \"R_threemonthssleepingmeds\"] \n",
    "\n",
    "#Prior Habits Parent: \n",
    "# subset = [\"R_priorweekdaybedtime\",\"R_priorweekendbedtime\",\"R_priorhoursofsleepweekdays\",\"R_priorhoursofsleepweekend\" ,\n",
    "#           \"R_exerciseprior\",\"R_outdoorsprior\", \"R_priortvmedia\",\"R_priorsocialmedia\", \"R_priorvideogames\", \n",
    "#           \"R_threemonthsalcohol\", \"R_threemonthsvaping\", \"R_threemonthstobacco\", \"R_threemonthsmarijuana\", \"R_threemonthsopiates\", \"R_threemonthsother\", \"R_threemonthssleepingmeds\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pull in data frame (CBCL.CSV) that contains subject ID, CBCL Scores, Age, and Sex\n",
    "df = pd.read_csv('C:/Users/jacob.derosa/Documents/Combined_Clustering/Clustering_Splits/ADULT_US_C_lf.csv') #Change file path depending on dataset \n",
    "df = df.rename(columns={'Unnamed: 0': 'Key'})\n",
    "\n",
    "#change subset variables depending on analysis\n",
    "subset = [\"R_positivechange\",\"R_inpersonconvo_bin\",\"R_timeoutside\",\"R_restrictionsstress\",\"R_contactschanged\",\"R_difficultydistancing\",\"R_familychange\",\"R_familychangestress\",\n",
    "          \"R_friendschange\",\"R_friendchangestress\",\"R_difficultycancellations\",\"R_financedifficulty\",\"R_livingdifficulty\",\"R_foodsecurity\"]  #Age and Sex Residuals \n",
    "y = np.array(df['Key'])\n",
    "n_straps = 10000 #number of bootsraps \n",
    "n = df.shape[0]\n",
    "b_idx = np.zeros((n_straps, n))\n",
    "\n",
    "# bootstrapping\n",
    "for i in range(n_straps):\n",
    "    random_state = np.random.RandomState()\n",
    "    b_idx[i] = random_state.randint(0, high=n - 1, size=n)\n",
    "\n",
    "b_idx = b_idx.astype(np.int)\n",
    "y_boot = np.zeros(b_idx.shape, dtype='object')\n",
    "for i in range(b_idx.shape[0]):\n",
    "    y_boot[i] = y[b_idx[i]]\n",
    "\n",
    "bootstrap_split_subids = []\n",
    "bootstrap_split_communities = []\n",
    "bootstrap_split_Q = []\n",
    "\n",
    "for i in range(n_straps):\n",
    "    X_split = df.iloc[b_idx[i],:]\n",
    "\n",
    "    bootstrap_split_subids.append([y_boot[i]])\n",
    "    \n",
    "    print('bootstrap #%d' % (i + 1), end='\\t')\n",
    "    communities, Q = pheno_clust(X=np.array(X_split[subset]).astype(np.float64), plot=False, verbose=False)\n",
    "    \n",
    "    bootstrap_split_communities.append([communities])\n",
    "    bootstrap_split_Q.append([Q])\n",
    "\n",
    "allcsvs = {}\n",
    "for i in range(n_straps):\n",
    "    df2 = {'Key':bootstrap_split_subids[i][0],'cluster':bootstrap_split_communities[i][0]}\n",
    "    allcsvs[i] = pd.DataFrame(df2)\n",
    "    allcsvs[i].to_csv('C:/Users/jacob.derosa/Documents/Combined_Clustering/Clusters/Adult/Data/Adult_US_%s.csv' % i) #change filepath depending on dataset/analysis\n",
    "    \n",
    "    \n",
    "csv_folder= 'C:/Users/jacob.derosa/Documents/Combined_Clustering/Clusters/Adult/Data/' #change filepath depending on dataset/analysis\n",
    "\n",
    "# Apply Bagging on Split 1 \n",
    "split = 'Adult_US' #Name of saved CSVs \n",
    "adj_full, mask_full, stab_full = bagging_adjacency_matrixes(csv_folder, split)\n",
    "\n",
    "#Create df Id from matrix column names\n",
    "columnsNamesArr = pd.DataFrame(stab_full.columns.values)\n",
    "\n",
    "# Final Louvain Clustering Split 1 \n",
    "graph = Graph.Weighted_Adjacency(stab_full.values.tolist(), mode=ADJ_UNDIRECTED, attr=\"weight\")\n",
    "Louvain = graph.community_multilevel(weights=graph.es['weight'])\n",
    "\n",
    "#Louvain.membership \n",
    "Q = graph.modularity(Louvain, weights=graph.es['weight'])\n",
    "print(Q)\n",
    "\n",
    "# Create dataframe of Subtypes for Split 1 \n",
    "Subtypes = pd.DataFrame(Louvain.membership)\n",
    "Subtypes = Subtypes + 1 \n",
    "\n",
    "# create dataframe of Split 1 Subtypes \n",
    "subs = pd.concat([columnsNamesArr.reset_index(drop=True), Subtypes], axis=1)\n",
    "subs.columns = ['studyid', 'Subtype']\n",
    "\n",
    "# read in Split 1 dataframe \n",
    "df = pd.read_csv('C:/Users/jacob.derosa/Documents/Combined_Clustering/Clustering_Splits/ADULT_US_C_lf.csv') # Change filepath depending on dataset/analysis\n",
    "# create Split 1 \n",
    "Split_1 = pd.merge(subs, df, on='Key')\n",
    "Split_1.to_csv('C:/Users/jacob.derosa/Documents/Combined_Clustering/Created_Clusters/Adult_UK_C_ls.csv') # Save new path"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
